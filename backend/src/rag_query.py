from typing import Generator, List
import os
import re
from dotenv import load_dotenv
from functools import lru_cache
from datetime import datetime
import psycopg2

from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain_community.vectorstores import PGVector
from langchain_community.chat_message_histories import PostgresChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.documents import Document

load_dotenv()

# ============================================================================
# Configuration
# ============================================================================
PG_USER = os.getenv("PG_USER")
PG_PASSWORD = os.getenv("PG_PASSWORD")
PG_HOST = os.getenv("PG_HOST")
PG_PORT = os.getenv("PG_PORT")
PG_DATABASE = os.getenv("PG_DATABASE")
COLLECTION_NAME = os.getenv("COLLECTION_NAME")

LLM_MODEL = os.getenv("LLM_MODEL", "qwen2.5:0.5b")
EMBED_MODEL = os.getenv("EMBED_MODEL", "nomic-embed-text")
OLLAMA_BASE_URL = "http://localhost:11434"

# ============================================================================
# Database Connection
# ============================================================================
SQLALCHEMY_DB_URL = (
    f"postgresql+psycopg2://"
    f"{PG_USER}:{PG_PASSWORD}@{PG_HOST}:{PG_PORT}/{PG_DATABASE}"
)

PSYCOPG_CONN_INFO = (
    f"dbname={PG_DATABASE} "
    f"user={PG_USER} "
    f"password={PG_PASSWORD} "
    f"host={PG_HOST} "
    f"port={PG_PORT}"
)

# ============================================================================
# Lazy Initialization
# ============================================================================
_embeddings = None
_vectorstore = None
_retriever = None
_llm = None
_db_conn = None

# ============================================================================
# Core Functions
# ============================================================================

def get_embeddings():
    global _embeddings
    if _embeddings is None:
        _embeddings = OllamaEmbeddings(
            model=EMBED_MODEL,
            base_url=OLLAMA_BASE_URL
        )
    return _embeddings

def get_vectorstore():
    global _vectorstore
    if _vectorstore is None:
        _vectorstore = PGVector(
            connection_string=SQLALCHEMY_DB_URL,
            collection_name=COLLECTION_NAME,
            embedding_function=get_embeddings(),
        )
    return _vectorstore

def get_retriever():
    global _retriever
    if _retriever is None:
        _vectorstore = get_vectorstore()
        _retriever = _vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={
                "k": 5,  # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 30 ‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 5 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß AI ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏¢‡∏≠‡∏∞
            }
        )
    return _retriever

# --- ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏™‡πà‡∏ß‡∏ô get_llm (‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà 84) ---
def get_llm():
    global _llm
    if _llm is None:
        _llm = ChatOllama(
            model=LLM_MODEL,
            temperature=0,  # ‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏õ‡πá‡∏ô 0 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏ï‡∏£‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô‡πÅ‡∏•‡∏∞‡∏•‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì
            stream=True,
            base_url=OLLAMA_BASE_URL,
            # ‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏π‡∏ô Context ‡πÅ‡∏•‡∏∞ Predict ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö RAM
            num_ctx=4096,    # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 8192 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î RAM
            num_predict=512, # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏û‡∏•‡πà‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏à‡∏ô‡∏Ñ‡πâ‡∏≤‡∏á
            repeat_penalty=1.2
        )
    return _llm
def get_db_connection():
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ PostgreSQL ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á"""
    global _db_conn
    if _db_conn is None or _db_conn.closed:
        _db_conn = psycopg2.connect(PSYCOPG_CONN_INFO)
    return _db_conn

# ============================================================================
# Keyword Extraction
# ============================================================================

def extract_search_patterns(question: str) -> dict:
    """‡∏î‡∏∂‡∏á‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Serial, Asset, Model ‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°"""
    
    patterns = {
        "serials": [],
        "assets": [],
        "models": [],
        "locations": [],
        "keywords": []
    }
    
    # ‡∏´‡∏≤ Serial Number (8+ ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ ‡∏ï‡∏±‡∏ß‡∏û‡∏¥‡∏°‡∏û‡πå‡πÉ‡∏´‡∏ç‡πà‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç)
    serials = re.findall(r'\b[a-zA-Z0-9-]{4,20}\b', question)
    patterns["serials"].extend([s.upper() for s in serials])
    
    # ‡∏´‡∏≤ Asset Number (7-8 ‡∏´‡∏•‡∏±‡∏Å)
    assets = re.findall(r'\b\d{7,10}\b', question)
    patterns["assets"].extend(assets)
    
    # ‡∏´‡∏≤ Model keywords
    model_keywords = ["thinkpad", "thinkcentre", "thinkstation", "switch", 
                      "router", "printer", "mac", "elitebook", "optiplex",
                      "g100", "6100", "neverstop"]
    
    q_lower = question.lower()
    for mk in model_keywords:
        if mk in q_lower:
            patterns["models"].append(mk)
    
    # ‡∏´‡∏≤ Location keywords
    location_keywords = ["sriracha", "‡∏®‡∏£‡∏µ‡∏£‡∏≤‡∏ä‡∏≤", "chonburi", "‡∏ä‡∏•‡∏ö‡∏∏‡∏£‡∏µ", 
                         "custom", "customs"]
    
    for lk in location_keywords:
        if lk in q_lower:
            patterns["locations"].append(lk)
    
    # General keywords
    if any(k in q_lower for k in ["spare", "‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ", "‡∏™‡∏≥‡∏£‡∏≠‡∏á"]):
        patterns["keywords"].append("spare")
    
    if any(k in q_lower for k in ["obsolete", "‡πÄ‡∏•‡∏¥‡∏Å‡πÉ‡∏ä‡πâ", "‡πÄ‡∏™‡∏∑‡πà‡∏≠‡∏°"]):
        patterns["keywords"].append("obsolete")
    
    return patterns

# ============================================================================
# Hybrid Retrieval - ‡∏™‡πà‡∏ß‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î!
# ============================================================================
def keyword_search_direct(patterns: dict) -> List[Document]:
    conn = get_db_connection()
    cursor = conn.cursor()
    all_docs = []

    search_terms = patterns["serials"] + patterns["assets"] + patterns["models"]
    
    try:
        for term in search_terms:
            query = """
            SELECT document, cmetadata
                FROM langchain_pg_embedding
                WHERE document ILIKE %s 
                   OR (cmetadata->>'Serial')::text ILIKE %s
                   OR (cmetadata->>'Asset No')::text ILIKE %s
                   OR (cmetadata->>'Model No.')::text ILIKE %s
                LIMIT 50 -- ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
            """
            cursor.execute(query, (f'%{term}%', f'%{term}%', f'%{term}%', f'%{term}%'))
            results = cursor.fetchall()
            
            print(f"[KEYWORD SEARCH] Term '{term}' found {len(results)} results")
            
            for doc_content, metadata in results:
                all_docs.append(
                    Document(
                        page_content=doc_content,
                        metadata=metadata or {}
                    )
                )

        
    except Exception as e:
        print(f"[KEYWORD SEARCH ERROR] {e}")
    finally:
        cursor.close()
    
    return all_docs
def hybrid_retrieve(question: str) -> List[Document]:
    print(f"\n[HYBRID RETRIEVAL] Question: {question}")

    # 1. Semantic Search (‡∏î‡∏∂‡∏á‡∏°‡∏≤‡∏ô‡πâ‡∏≠‡∏¢‡πÜ)
    retriever = get_retriever()
    semantic_docs = retriever.invoke(question)
    
    # 2. Keyword Search (‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡∏à‡∏£‡∏¥‡∏á‡πÜ)
    patterns = extract_search_patterns(question)
    keyword_docs = []
    if any(patterns.values()):
        keyword_docs = keyword_search_direct(patterns)
    
    # ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    all_docs = keyword_docs + semantic_docs
    
    # ‡∏•‡∏ö‡∏ã‡πâ‡∏≥
    seen = set()
    unique_docs = []
    
    for doc in all_docs:
        # ‡πÉ‡∏ä‡πâ Serial ‡πÄ‡∏õ‡πá‡∏ô Key ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡∏ã‡πâ‡∏≥‡∏à‡∏∞‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Å‡∏ß‡πà‡∏≤
        serial = doc.metadata.get('Serial', doc.page_content[:100])
        if serial not in seen:
            seen.add(serial)
            unique_docs.append(doc)
    
    # ‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ AI ‡πÅ‡∏Ñ‡πà 5-10 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÅ‡∏£‡∏Å‡∏û‡∏≠ (‡∏•‡∏î‡∏à‡∏≤‡∏Å 30)
    final_docs = unique_docs[:10]
    print(f"[HYBRID RESULT] Sent {len(final_docs)} unique docs to LLM")
    
    return unique_docs[:10]

# ============================================================================
# Enhanced Prompts
# ============================================================================

IT_ASSET_PROMPT = ChatPromptTemplate.from_template("""
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ AI IT Support Assistant ‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ IT Asset

## ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏ö:
{context}

## ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô: {current_date}

## ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:
{question}

## ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö:
1. **‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î** - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ó‡∏∏‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£
2. **‡∏ï‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô** - ‡∏≠‡∏¢‡πà‡∏≤‡πÄ‡∏î‡∏≤ ‡∏≠‡∏¢‡πà‡∏≤‡πÅ‡∏ï‡πà‡∏á‡πÄ‡∏ï‡∏¥‡∏°
3. **‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢** - ‡πÉ‡∏ä‡πâ emoji, bullet points, ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
4. **‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£** - ‡πÅ‡∏™‡∏î‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 5 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÅ‡∏£‡∏Å
5. **‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠** - ‡∏ö‡∏≠‡∏Å‡∏ï‡∏£‡∏á‡πÜ‡∏ß‡πà‡∏≤ "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"

## ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏î‡∏µ:

**‡∏ñ‡∏≤‡∏°‡∏´‡∏≤ Serial:**
```
üîç ‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Serial TW37KNP21D

üì¶ ‡∏£‡∏∏‡πà‡∏ô: 6100 12G Class4 PoE 2G/2SF+ 139W Switch
üî¢ Model No: HPE-JL679A
üè∑Ô∏è Serial: TW37KNP21D
üíº Asset No: 10029034
‚úÖ ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞: Spare (‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô)
üìç ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á: Sriracha
```

**‡∏ñ‡∏≤‡∏°‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô:**
```
üìä ‡∏°‡∏µ ThinkPad ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î 12 ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á

‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î:
‚úÖ Spare: 4 ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á
‚ö†Ô∏è Obsolete: 8 ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á

‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ Spare:
1. T480 - S/N: ABC123 - Asset: 10001234
2. T490 - S/N: DEF456 - Asset: 10001235
...
```

‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:
""")

GENERAL_PROMPT = ChatPromptTemplate.from_template("""
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ AI IT Support Assistant ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£

‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà: {current_date}

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}

‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á):
""")

# ============================================================================
# Chat History
# ============================================================================

@lru_cache(maxsize=10)
def get_session_history(session_id: str):
    return PostgresChatMessageHistory(
        connection_string=PSYCOPG_CONN_INFO,
        session_id=session_id
    )

# ============================================================================
# Intent Classification
# ============================================================================

IT_ASSET_KEYWORDS = [
    "serial", "s/n", "sn", "asset", "model", "‡∏£‡∏∏‡πà‡∏ô", "‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á", "‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå",
    "‡∏°‡∏µ", "‡πÄ‡∏´‡∏•‡∏∑‡∏≠", "‡∏Å‡∏µ‡πà", "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô", "spare", "obsolete", "‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤", "‡∏´‡∏≤",
    "thinkpad", "laptop", "switch", "router", "printer", "computer",
    "location", "‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á", "‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà", "sriracha", "‡∏®‡∏£‡∏µ‡∏£‡∏≤‡∏ä‡∏≤", "Model No" ,
    "model no", "asset no", "asset no.", "serial number"
]

def classify_intent(question: str) -> str:
    q_lower = question.lower()
    
    # ‡∏°‡∏µ Serial/Asset pattern = ‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô IT Asset
    if re.search(r'[A-Z0-9]{7,}', question):
        return "it_asset"
    
    # ‡∏°‡∏µ keywords
    if any(k in q_lower for k in IT_ASSET_KEYWORDS):
        return "it_asset"
    
    return "general"

# ============================================================================
# Context Formatting
# ============================================================================
# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô format_context_for_llm
def format_context_for_llm(docs, max_docs: int = 50) -> str:
    if not docs:
        return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
    
    docs = docs[:max_docs]
    parts = [f"‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {len(docs)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á", ""]
    
    for i, doc in enumerate(docs, 1):
        meta = doc.metadata
        
        # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡πÅ‡∏ö‡∏ö Case-insensitive ‡πÅ‡∏•‡∏∞‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏Å
        def get_val(keys_list):
            low_meta = {k.lower(): v for k, v in meta.items()}
            for k in keys_list:
                if k in meta: return meta[k]
                if k.lower() in low_meta: return low_meta[k.lower()]
            return "N/A"

        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ï‡∏≤‡∏°‡∏´‡∏±‡∏ß‡∏ï‡∏≤‡∏£‡∏≤‡∏á Excel
        model_name = get_val(['model', 'Model Name'])
        model_no = get_val(['model no.', 'model no', 'model_no'])
        serial = get_val(['serial', 'serial number', 's/n'])
        asset = get_val(['asset no', 'asset no.', 'asset_no'])
        status = get_val(['status'])
        location = get_val(['location', 'locations'])

        parts.append(f"### ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà {i}:")
        parts.append(f"- ‡∏£‡∏∏‡πà‡∏ô: {model_name} (Model No: {model_no})")
        parts.append(f"- Serial: {serial}")
        parts.append(f"- Asset No: {asset}")
        parts.append(f"- ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞: {status}")
        parts.append(f"- ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á: {location}")
        parts.append(f"‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°: {doc.page_content}")
        parts.append("-" * 30)
    
    return "\n".join(parts)
# ============================================================================
# Main Chat Function
# ============================================================================

def chat_with_warehouse_system(
    session_id: str,
    question: str,
    image: bytes | None = None
) -> Generator[str, None, None]:
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å"""
    
    llm = get_llm()
    history = get_session_history(session_id)
    current_date = datetime.now().strftime("%Y-%m-%d")
    
    intent = classify_intent(question)
    print(f"\n[INTENT] {intent}")
    
    # IT ASSET MODE
    if intent == "it_asset":
        # ‡πÉ‡∏ä‡πâ Hybrid Retrieval
        docs = hybrid_retrieve(question)
        
        if not docs:
            yield "üîç ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö\n\n"
            yield "üí° ‡∏•‡∏≠‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö:\n"
            yield "‚Ä¢ Serial Number ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\n"
            yield "‚Ä¢ ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏î‡πâ‡∏ß‡∏¢ Model ‡∏´‡∏£‡∏∑‡∏≠ Asset Number\n"
            return
        
        context = format_context_for_llm(docs)
        
        chain = (
            {
                "context": lambda _: context,
                "question": RunnablePassthrough(),
                "current_date": lambda _: current_date
            }
            | IT_ASSET_PROMPT
            | llm
        )
        
        full_response = ""
        for chunk in chain.stream(question):
            content = getattr(chunk, "content", str(chunk))
            full_response += content
            yield content
        
        history.add_user_message(question)
        history.add_ai_message(full_response)
        return
    
    # GENERAL MODE
    chain = (
        {
            "question": RunnablePassthrough(),
            "current_date": lambda _: current_date
        }
        | GENERAL_PROMPT
        | llm
    )
    
    full_response = ""
    for chunk in chain.stream(question):
        content = getattr(chunk, "content", str(chunk))
        full_response += content
        yield content
    
    history.add_user_message(question)
    history.add_ai_message(full_response)

# ============================================================================
# Utilities
# ============================================================================

def clear_session_history(session_id: str):
    history = get_session_history(session_id)
    history.clear()
    get_session_history.cache_clear()

def cleanup_resources():
    global _vectorstore, _embeddings, _llm, _retriever, _db_conn
    if _db_conn and not _db_conn.closed:
        _db_conn.close()
    _vectorstore = None
    _embeddings = None
    _llm = None
    _retriever = None
    _db_conn = None
    get_session_history.cache_clear()